{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYihWU0Z2j0P"
      },
      "source": [
        "                 Logistic Regression Assignment\n",
        "\n",
        "1.What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "\n",
        " .Logistic Regression vs. Linear Regression\n",
        "\n",
        ". Logistic Regression:\n",
        "Logistic Regression is a statistical method used for classification problems.\n",
        "\n",
        "It predicts probabilities that map inputs to a binary or multi-class outcome.\n",
        "\n",
        "The model applies the sigmoid (logistic) function to transform the linear equation output into a probability value between 0 and 1.\n",
        "\n",
        "The decision boundary is determined by setting a threshold (e.g., 0.5) to classify the outcome.\n",
        "\n",
        ". Linear Regression:\n",
        "\n",
        "Linear Regression is used for regression problems, where the output is continuous.\n",
        "\n",
        "It finds the best-fitting line by minimizing the error (using methods like Ordinary Least Squares).\n",
        "\n",
        "The output is a continuous numerical value without probability constraints.\n",
        "\n",
        "2.What is the mathematical equation of Logistic Regression.\n",
        "\n",
        " . The mathematical equation of Logistic Regression is based on the sigmoid function applied to a linear model.\n",
        "\n",
        " Linear Equation (Before Applying Sigmoid)\n",
        "\n",
        "Logistic regression first computes a linear combination of input features:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝑤\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝑤\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑤\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "+\n",
        "𝑏\n",
        "z=w\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +w\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+w\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " +b\n",
        "\n",
        "or in vector form:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝑤\n",
        "𝑇\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "z=w\n",
        "T\n",
        " x+b\n",
        "\n",
        "where:\n",
        "\n",
        "𝑥\n",
        "=\n",
        "[\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "]\n",
        "x=[x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " ] is the input feature vector.\n",
        "\n",
        "𝑤\n",
        "=\n",
        "[\n",
        "𝑤\n",
        "1\n",
        ",\n",
        "𝑤\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑤\n",
        "𝑛\n",
        "]\n",
        "w=[w\n",
        "1\n",
        "​\n",
        " ,w\n",
        "2\n",
        "​\n",
        " ,…,w\n",
        "n\n",
        "​\n",
        " ] is the weight vector.\n",
        "\n",
        "𝑏\n",
        "b is the bias term.\n",
        "\n",
        "3.Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        ". Why Do We Use the Sigmoid Function in Logistic Regression?\n",
        "\n",
        "The sigmoid function is used in logistic regression because it transforms the output of a linear equation into a probability value between 0 and 1, making it suitable for binary classification problems.\n",
        "\n",
        ". Key Reasons for Using the Sigmoid Function:\n",
        "\n",
        "Probability Interpretation:\n",
        "\n",
        "The sigmoid function outputs values in the range (0,1), which can be directly interpreted as a probability.\n",
        "\n",
        "This helps in deciding class labels using a probability threshold (e.g., 0.5).\n",
        "\n",
        "Non-linearity:\n",
        "\n",
        "Even though logistic regression is a linear model, applying the sigmoid function introduces non-linearity in the decision boundary, making it useful for classification.\n",
        "\n",
        "Differentiability:\n",
        "\n",
        "The sigmoid function is smooth and differentiable, allowing gradient-based optimization methods (e.g., Gradient Descent) to optimize the model.\n",
        "\n",
        "4.What is the cost function of Logistic Regression?\n",
        "\n",
        ".The cost function of Logistic Regression is based on the log-likelihood function (also called the log loss or binary cross-entropy loss). It is designed to measure how well the logistic regression model's predictions match the actual labels.\n",
        "\n",
        "Why Not Use Mean Squared Error (MSE)?\n",
        "\n",
        "Using MSE for logistic regression is not ideal because:\n",
        "\n",
        "The sigmoid function is nonlinear, leading to a non-convex cost function, which makes gradient descent optimization difficult.\n",
        "\n",
        "Cross-entropy loss ensures a convex cost function, leading to better convergence with gradient descent.\n",
        "\n",
        "5.What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        ".Regularization in Logistic Regression\n",
        "\n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function in Logistic Regression. This penalty discourages large values of the model parameters (θ) and helps improve generalization.\n",
        "\n",
        "Types of Regularization in Logistic Regression\n",
        "L1 Regularization (Lasso Regression)\n",
        "\n",
        "Adds the absolute values of the coefficients as a penalty.\n",
        "\n",
        "Encourages sparsity by shrinking some coefficients to zero, effectively selecting important features.\n",
        "\n",
        "6.Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
        "\n",
        ". Difference Between Lasso, Ridge, and Elastic Net Regression\n",
        "Lasso, Ridge, and Elastic Net are types of regularization techniques used to prevent overfitting by adding a penalty term to the loss function in regression models.\n",
        "\n",
        "The key difference among them lies in how they penalize the model coefficients.\n",
        "\n",
        ". Ridge Regression (L2 Regularization)\n",
        "\n",
        "Penalty Term: Adds the sum of squared coefficients to the loss function.\n",
        "\n",
        "Effect:\n",
        "\n",
        "Shrinks coefficients towards zero but does not eliminate any.\n",
        "\n",
        "Helps in handling multicollinearity by reducing the impact of correlated variables.\n",
        "\n",
        "Produces a stable model with all features retained.\n",
        "\n",
        "Best Use Case: When all features are important and collinearity exists in the data.\n",
        "\n",
        ". Lasso Regression (L1 Regularization)\n",
        "\n",
        "Penalty Term: Adds the sum of absolute values of coefficients to the loss function.\n",
        "\n",
        " Effect:\n",
        "\n",
        "Can shrink some coefficients to zero, effectively selecting important features and ignoring the rest.\n",
        "\n",
        "Helps in feature selection, making the model more interpretable.\n",
        "\n",
        "Best Use Case: When feature selection is required, as it automatically drops irrelevant variables.\n",
        "\n",
        ". Elastic Net Regression (Combination of L1 & L2)\n",
        "\n",
        "Penalty Term: Combines both L1 and L2 regularization.\n",
        "\n",
        "Effect:\n",
        "\n",
        "Handles multicollinearity (like Ridge).\n",
        "\n",
        "Performs feature selection (like Lasso) but in a more stable way.\n",
        "\n",
        "Useful when there are many correlated features.\n",
        "\n",
        " Best Use Case: When Lasso is too aggressive in eliminating features and Ridge alone is not sufficient.\n",
        "\n",
        "7.When should we use Elastic Net instead of Lasso or Ridge.\n",
        "\n",
        ".When to Use Elastic Net Instead of Lasso or Ridge?\n",
        "Elastic Net is a hybrid of Lasso (L1) and Ridge (L2) regularization, combining their strengths while addressing their limitations. You should use Elastic Net instead of Lasso or Ridge in the following scenarios:\n",
        "\n",
        "8.What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "\n",
        ". Impact of the Regularization Parameter (λ) in Logistic Regression\n",
        "The regularization parameter λ (lambda) in logistic regression controls the strength of the penalty applied to the model’s coefficients. It plays a crucial role in balancing bias and variance, impacting model complexity and performance.\n",
        "\n",
        ". When λ is Too Small (Weak Regularization)\n",
        "\n",
        " Effect:\n",
        "\n",
        "The penalty term is small or negligible.\n",
        "\n",
        "The model tries to fit the training data perfectly, leading to overfitting.\n",
        "\n",
        "Large coefficients, making the model more complex and sensitive to noise.\n",
        "\n",
        " Risk: High variance, meaning poor generalization to new data.\n",
        "\n",
        " Best for: When the dataset is small, clean, and has a strong underlying pattern.\n",
        "\n",
        " 9.What are the key assumptions of Logistic Regression.\n",
        "\n",
        " .Key Assumptions of Logistic Regression\n",
        "\n",
        "Logistic Regression is a widely used classification algorithm, but it relies on several key assumptions for optimal performance. Unlike Linear Regression, it models the probability of an event occurring using the logit function.\n",
        "\n",
        "10.What are some alternatives to Logistic Regression for classification tasks.\n",
        "\n",
        ". Alternatives to Logistic Regression for Classification Tasks\n",
        "While Logistic Regression is a great baseline classifier, it has limitations, especially with non-linear relationships, high-dimensional data, and complex decision boundaries.\n",
        "\n",
        "11.What are Classification Evaluation Metrics.\n",
        "\n",
        ". Classification Evaluation Metrics\n",
        "\n",
        "When evaluating a classification model, it’s important to use the right metrics to measure performance accurately. Here are the most commonly used classification evaluation metrics:\n",
        "\n",
        ". Accuracy\n",
        "\n",
        ". Precision (Positive Predictive Value)\n",
        "\n",
        "12.How does class imbalance affect Logistic Regression.\n",
        "\n",
        "How Class Imbalance Affects Logistic Regression\n",
        "\n",
        "Class imbalance occurs when one class is significantly more frequent than the other in a dataset. This can negatively impact Logistic Regression (and other classifiers) by causing biased predictions and misleading performance metrics.\n",
        "\n",
        "13.What is Hyperparameter Tuning in Logistic Regression.\n",
        "\n",
        ". Hyperparameter Tuning in Logistic Regression\n",
        "Hyperparameter tuning is the process of optimizing the settings of a machine learning model to achieve the best performance. In Logistic Regression, hyperparameters control regularization, optimization, and model complexity.\n",
        "\n",
        "14.What are different solvers in Logistic Regression? Which one should be used.\n",
        "\n",
        ". Different Solvers in Logistic Regression & When to Use Them\n",
        "Logistic Regression uses optimization algorithms (solvers) to find the best model parameters. The choice of solver affects speed, convergence, and compatibility with different regularization techniques.\n",
        "\n",
        "15.ow is Logistic Regression extended for multiclass classification.\n",
        "\n",
        ". Extending Logistic Regression for Multiclass Classification\n",
        "By default, Logistic Regression is designed for binary classification (0 or 1). To handle multiclass problems (3 or more classes), it can be extended using two main strategies:\n",
        "\n",
        ". 1. One-vs-Rest (OvR) Approach (Default in Scikit-Learn)\n",
        "\n",
        ". 2. One-vs-One (OvO) Approach\n",
        "\n",
        "16.What are the advantages and disadvantages of Logistic Regression.\n",
        "\n",
        ". Advantages & Disadvantages of Logistic Regression\n",
        "\n",
        "Logistic Regression is one of the most widely used classification algorithms, but it has strengths and limitations depending on the dataset and problem.\n",
        "\n",
        ". Advantages of Logistic Regression\n",
        "\n",
        ". Simple & Easy to Implement\n",
        "\n",
        ". Probabilistic Interpretation\n",
        "\n",
        ". Disadvantages of Logistic Regression\n",
        "\n",
        ".  Assumes Linearity Between Features & Log-Odds\n",
        "\n",
        ". Struggles with High-Dimensional Data\n",
        "\n",
        ". Poor Performance on Imbalanced Data\n",
        "\n",
        "17.What are some use cases of Logistic Regression.\n",
        "\n",
        ". Real-World Use Cases of Logistic Regression\n",
        "\n",
        "Logistic Regression is widely used for binary and multiclass classification in various industries. Its simplicity, interpretability, and efficiency make it a go-to model for many practical applications.\n",
        "\n",
        ". Medical Diagnosis & Disease Prediction\n",
        "\n",
        ". Spam Detection (Email & SMS Classification)\n",
        "\n",
        "18.What is the difference between Softmax Regression and Logistic Regression.\n",
        "\n",
        ". Softmax Regression vs. Logistic Regression\n",
        "\n",
        "Both Logistic Regression and Softmax Regression are used for classification, but they serve different purposes in terms of the number of classes they handle.\n",
        "\n",
        "19.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "\n",
        ". Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification\n",
        "When dealing with multiclass classification (more than two classes), there are two common approaches:\n",
        ". One-vs-Rest (OvR) (a.k.a. One-vs-All, OvA)\n",
        ". Softmax Regression (Multinomial Logistic Regression)\n",
        "\n",
        "Each has its strengths and weaknesses, so choosing the right one depends on dataset size, interpretability, and computational efficiency.\n",
        "\n",
        "20.How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        ". Interpreting Coefficients in Logistic Regression\n",
        "\n",
        "In Logistic Regression, the coefficients (\n",
        "𝛽\n",
        "β) represent the impact of each feature on the log-odds of the target outcome. Unlike Linear Regression, where coefficients directly indicate changes in the dependent variable, Logistic Regression uses the logistic (sigmoid) function to transform predictions into probabilities.\n",
        "\n",
        "                           Practical\n",
        "\n",
        "1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIIz7-lVMU2t",
        "outputId": "fba5931f-4241-4f50-c4bb-3de639eac650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# 1️⃣ Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = (iris.target == 2).astype(int)  # Binary classification: \"Is it Iris Virginica?\"\n",
        "\n",
        "# 2️⃣ Split into Training and Testing sets (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3️⃣ Apply Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4️⃣ Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5️⃣ Evaluate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")  # Print accuracy as a percentage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ektToRNLMevW"
      },
      "source": [
        "2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcfTRv7GMlfh",
        "outputId": "6face7ea-3c0e-4307-f477-105ab172f48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n",
            "L1 Regularized Coefficients: [[ 0.          1.17849016 -4.14196797  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Convert to binary classification (setosa vs others)\n",
        "y = (y == 0).astype(int)  # Binary classification: 1 if setosa, else 0\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Display coefficients (L1 regularization leads to sparsity)\n",
        "print(\"L1 Regularized Coefficients:\", model.coef_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
      ],
      "metadata": {
        "id": "esnZPK9_c5Xh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd1B-CvG2EQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473e8432-5003-4e98-c592-8dbd20c7f6a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n",
            "L2 Regularized Coefficients: [[-0.77929311  1.3519912  -1.59627349 -1.42737302]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Convert to binary classification (setosa vs others)\n",
        "y = (y == 0).astype(int)  # 1 if setosa, else 0\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Display coefficients\n",
        "print(\"L2 Regularized Coefficients:\", model.coef_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n"
      ],
      "metadata": {
        "id": "mBZa4DSzdS_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Convert to binary classification (setosa vs others)\n",
        "y = (y == 0).astype(int)  # 1 if setosa, else 0\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply Elastic Net regularization (combination of L1 and L2)\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', C=1.0, l1_ratio=0.5)  # 50% L1, 50% L2\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Elastic Net Regularized Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTpkjHXZdW_H",
        "outputId": "d4febf69-7cb3-4bbd-e731-7bbef1e38f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n",
            "Elastic Net Regularized Coefficients: [[-0.79953835  1.12791614 -1.90722208 -1.66129654]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'"
      ],
      "metadata": {
        "id": "eTpNbXjVdkUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Multiclass labels (0, 1, 2)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with OvR strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', penalty='l2', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Model Coefficients:\\n\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AAGFHW9dJIY",
        "outputId": "8f9ea59d-e3d4-4d05-a5f2-03de444861e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9667\n",
            "Model Coefficients:\n",
            " [[-0.77929311  1.3519912  -1.59627349 -1.42737302]\n",
            " [ 0.25113137 -1.26696209  0.55078399 -0.73931909]\n",
            " [ 0.0180311  -0.20827858  1.73529514  2.39229869]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "YO8U054vdy8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Multiclass labels (0, 1, 2)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(solver='saga', max_iter=5000)  # 'saga' supports l1, l2, and elasticnet\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', None],  # Different penalty types\n",
        "    'l1_ratio': [0.5]  # Only used when penalty='elasticnet'\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict using best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Accuracy with Best Model: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF9HCoX9duFf",
        "outputId": "65f90c18-dd41-4da1-8c6e-6013ba4e8427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'l1_ratio': 0.5, 'penalty': 'l2'}\n",
            "Accuracy with Best Model: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "wr9fYjWad_8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Multiclass labels (0, 1, 2)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Define Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', penalty='l2', C=1.0)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(f'Cross-Validation Accuracies: {scores}')\n",
        "print(f'Average Accuracy: {np.mean(scores):.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL1cIgL7d8xR",
        "outputId": "38eb4a69-ddb9-48d5-d47b-ae47cc45744d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracies: [0.93333333 0.96666667 0.8        0.93333333 0.86666667]\n",
            "Average Accuracy: 0.9000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "maplAUGnelyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file (update 'data.csv' with your actual file path)\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = df.iloc[:, :-1].values  # Features (all columns except the last)\n",
        "y = df.iloc[:, -1].values   # Target variable (last column)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (optional but recommended for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "CMuKYYhKexhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "YOW6uVCee3Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Multiclass classification\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=5000)\n",
        "\n",
        "# Define hyperparameter distribution\n",
        "param_dist = {\n",
        "    'C': loguniform(0.01, 100),  # Random values for regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Different penalty types\n",
        "    'solver': ['liblinear', 'saga'],  # Supported solvers for L1, L2, and ElasticNet\n",
        "    'l1_ratio': [0.1, 0.5, 0.9]  # Only used for ElasticNet\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(model, param_dist, n_iter=20, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predict using best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Accuracy with Best Model: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b9SJOkCfAoc",
        "outputId": "130d4cac-c8f4-4869-a6eb-427e5eecd73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': np.float64(23.395864551222477), 'l1_ratio': 0.5, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Accuracy with Best Model: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "5 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.95       0.825      0.85       0.95       0.825\n",
            " 0.94166667 0.90833333 0.88333333 0.95       0.93333333 0.94166667\n",
            " 0.95       0.88333333 0.86666667 0.34166667 0.85833333 0.83333333\n",
            " 0.95833333 0.95      ]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n"
      ],
      "metadata": {
        "id": "xl45j8TBfOQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Multiclass labels (0, 1, 2)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with One-vs-One (OvO) strategy\n",
        "model = LogisticRegression(multi_class='ovo', solver='liblinear', penalty='l2', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "jhcBqM4NfGlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "LIhn4rlTpaOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
        "                           n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize and train the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# 5. Generate and print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 6. Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# 7. Visualize the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=log_reg.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix for Logistic Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "zRV-4i-vpt7_",
        "outputId": "aacfc450-d343-4790-eb74-b9dfbc0a5133"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93       156\n",
            "           1       0.93      0.91      0.92       144\n",
            "\n",
            "    accuracy                           0.92       300\n",
            "   macro avg       0.92      0.92      0.92       300\n",
            "weighted avg       0.92      0.92      0.92       300\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score."
      ],
      "metadata": {
        "id": "ZYM6ubKOqAtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_classes=2,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 3. Initialize and train the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# 5. Calculate evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# 6. Print the individual metrics\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# 7. Print the comprehensive classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 8. Print the model coefficients (optional)\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(log_reg.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UudayT5UpvKX",
        "outputId": "39937799-f0b4-44f5-e557-6fb342b71b39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation Metrics:\n",
            "Precision: 0.9291\n",
            "Recall: 0.9097\n",
            "F1-Score: 0.9193\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93       156\n",
            "           1       0.93      0.91      0.92       144\n",
            "\n",
            "    accuracy                           0.92       300\n",
            "   macro avg       0.92      0.92      0.92       300\n",
            "weighted avg       0.92      0.92      0.92       300\n",
            "\n",
            "\n",
            "Model Coefficients:\n",
            "[[ 0.37277247  0.05791831  0.25590869  0.05233168  1.91058091 -1.39730391\n",
            "   0.15677771  0.03449757 -0.12958115  0.01976966]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "z1bHmf3RqexL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## 1. Create an imbalanced dataset\n",
        "# Generate synthetic data with 95:5 class imbalance\n",
        "X, y = make_classification(n_samples=10000, n_features=10, n_classes=2,\n",
        "                           n_clusters_per_class=1, weights=[0.95, 0.05],\n",
        "                           random_state=42)\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "df = pd.DataFrame(X)\n",
        "df['target'] = y\n",
        "\n",
        "# Show class distribution\n",
        "print(\"Class Distribution:\")\n",
        "print(df['target'].value_counts())\n",
        "print(\"\\nClass Percentage:\")\n",
        "print(df['target'].value_counts(normalize=True) * 100)\n",
        "\n",
        "## 2. Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "## 3. Train a baseline model without class weights\n",
        "print(\"\\nTraining Baseline Model (without class weights)...\")\n",
        "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "baseline_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate baseline model\n",
        "y_pred = baseline_model.predict(X_test)\n",
        "print(\"\\nBaseline Model Performance:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "## 4. Train a model with class weights (balanced)\n",
        "print(\"\\nTraining Model with Class Weights...\")\n",
        "# Option 1: Automatically balance weights\n",
        "weighted_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
        "\n",
        "# Option 2: Manually specify weights (uncomment to use)\n",
        "# class_weights = {0: 0.05, 1: 0.95}  # Inverse of class frequencies\n",
        "# weighted_model = LogisticRegression(max_iter=1000, class_weight=class_weights, random_state=42)\n",
        "\n",
        "weighted_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate weighted model\n",
        "y_pred_weighted = weighted_model.predict(X_test)\n",
        "print(\"\\nWeighted Model Performance:\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n",
        "\n",
        "## 5. Visual comparison of both models\n",
        "# Confusion matrix for baseline model\n",
        "print(\"\\nConfusion Matrix - Baseline Model\")\n",
        "cm_baseline = confusion_matrix(y_test, y_pred)\n",
        "disp_baseline = ConfusionMatrixDisplay(confusion_matrix=cm_baseline,\n",
        "                                      display_labels=baseline_model.classes_)\n",
        "disp_baseline.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Baseline Model\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix for weighted model\n",
        "print(\"\\nConfusion Matrix - Weighted Model\")\n",
        "cm_weighted = confusion_matrix(y_test, y_pred_weighted)\n",
        "disp_weighted = ConfusionMatrixDisplay(confusion_matrix=cm_weighted,\n",
        "                                      display_labels=weighted_model.classes_)\n",
        "disp_weighted.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Weighted Model\")\n",
        "plt.show()\n",
        "\n",
        "## 6. Compare prediction probabilities\n",
        "# Get prediction probabilities for both models\n",
        "y_proba_baseline = baseline_model.predict_proba(X_test)[:, 1]\n",
        "y_proba_weighted = weighted_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Create a DataFrame to compare probabilities\n",
        "prob_df = pd.DataFrame({\n",
        "    'True_Label': y_test,\n",
        "    'Baseline_Prob': y_proba_baseline,\n",
        "    'Weighted_Prob': y_proba_weighted\n",
        "})\n",
        "\n",
        "# Show probabilities for some minority class samples\n",
        "print(\"\\nProbability Comparison for Minority Class Samples:\")\n",
        "print(prob_df[prob_df['True_Label'] == 1].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wkTed8itqdYm",
        "outputId": "fbed5408-0e48-4a42-9957-39976f37a98c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution:\n",
            "target\n",
            "0    9453\n",
            "1     547\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class Percentage:\n",
            "target\n",
            "0    94.53\n",
            "1     5.47\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Training Baseline Model (without class weights)...\n",
            "\n",
            "Baseline Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      2836\n",
            "           1       0.99      0.82      0.90       164\n",
            "\n",
            "    accuracy                           0.99      3000\n",
            "   macro avg       0.99      0.91      0.95      3000\n",
            "weighted avg       0.99      0.99      0.99      3000\n",
            "\n",
            "\n",
            "Training Model with Class Weights...\n",
            "\n",
            "Weighted Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      2836\n",
            "           1       0.65      0.90      0.76       164\n",
            "\n",
            "    accuracy                           0.97      3000\n",
            "   macro avg       0.82      0.93      0.87      3000\n",
            "weighted avg       0.98      0.97      0.97      3000\n",
            "\n",
            "\n",
            "Confusion Matrix - Baseline Model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix - Weighted Model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Probability Comparison for Minority Class Samples:\n",
            "     True_Label  Baseline_Prob  Weighted_Prob\n",
            "11            1       0.976360       0.987988\n",
            "51            1       0.778316       0.944505\n",
            "134           1       0.987279       0.992394\n",
            "176           1       0.218075       0.679042\n",
            "177           1       0.981073       0.990658\n",
            "196           1       0.762380       0.939835\n",
            "201           1       0.793741       0.937467\n",
            "204           1       0.941827       0.968611\n",
            "229           1       0.072757       0.453326\n",
            "231           1       0.999300       0.999211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XexO7fgqq8Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Load the Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "# Display basic info\n",
        "print(\"Dataset Info:\")\n",
        "print(titanic_df.info())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(titanic_df.isnull().sum())\n",
        "\n",
        "# 2. Feature selection and preprocessing\n",
        "# Select relevant features\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "X = titanic_df[features]\n",
        "y = titanic_df[target]\n",
        "\n",
        "# 3. Create preprocessing pipelines\n",
        "# Numerical features pipeline\n",
        "num_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "num_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical features pipeline\n",
        "cat_features = ['Pclass', 'Sex', 'Embarked']\n",
        "cat_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_transformer, num_features),\n",
        "    ('cat', cat_transformer, cat_features)\n",
        "])\n",
        "\n",
        "# 4. Create and train the model pipeline\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"\\nModel Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Died', 'Survived'])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# 6. Feature importance (for numeric features)\n",
        "# Get coefficients from the logistic regression model\n",
        "log_reg = model.named_steps['classifier']\n",
        "feature_names = (model.named_steps['preprocessor']\n",
        "                 .named_transformers_['cat']\n",
        "                 .named_steps['encoder']\n",
        "                 .get_feature_names_out(cat_features))\n",
        "feature_names = np.concatenate([num_features, feature_names])\n",
        "\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': log_reg.coef_[0]\n",
        "}).sort_values('Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(coef_df)\n",
        "\n",
        "# 7. Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Coefficient', y='Feature', data=coef_df)\n",
        "plt.title('Logistic Regression Coefficients')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zypJByKnr84g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "Gs5euKfDsGyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the breast cancer dataset (good for binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "## Model 1: Logistic Regression without scaling\n",
        "print(\"Training Logistic Regression WITHOUT feature scaling...\")\n",
        "model_no_scale = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "print(\"\\nResults WITHOUT scaling:\")\n",
        "print(f\"Accuracy: {accuracy_no_scale:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_no_scale))\n",
        "\n",
        "## Model 2: Logistic Regression with Standardization\n",
        "print(\"\\nTraining Logistic Regression WITH feature scaling...\")\n",
        "\n",
        "# Initialize and fit the scaler on training data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scale = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_with_scale.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_with_scale = model_with_scale.predict(X_test_scaled)\n",
        "accuracy_with_scale = accuracy_score(y_test, y_pred_with_scale)\n",
        "\n",
        "print(\"\\nResults WITH scaling:\")\n",
        "print(f\"Accuracy: {accuracy_with_scale:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_with_scale))\n",
        "\n",
        "## Comparison\n",
        "print(\"\\nComparison Summary:\")\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_with_scale:.4f}\")\n",
        "print(f\"Difference:              {accuracy_with_scale - accuracy_no_scale:.4f}\")\n",
        "\n",
        "# Show coefficients to demonstrate scaling impact\n",
        "print(\"\\nCoefficient Comparison (first 5 features):\")\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': feature_names[:5],\n",
        "    'Without Scaling': model_no_scale.coef_[0][:5],\n",
        "    'With Scaling': model_with_scale.coef_[0][:5]\n",
        "})\n",
        "print(coef_df)"
      ],
      "metadata": {
        "id": "gpICDhpLsPwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.EIM Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n"
      ],
      "metadata": {
        "id": "HYs4N3Hxsi3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
        "\n",
        "## 1. Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_informative=10, random_state=42)\n",
        "\n",
        "## 2. Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "## 3. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "## 4. Make predictions and calculate probabilities\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "## 5. Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "## 6. Generate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "## 7. Additional performance metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "## 8. Find optimal threshold (Youden's J statistic)\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Make predictions using optimal threshold\n",
        "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "print(\"\\nPerformance with Optimal Threshold:\")\n",
        "print(classification_report(y_test, y_pred_optimal))"
      ],
      "metadata": {
        "id": "QbLLA4YStOxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "IiQTkNTTtQFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## 1. Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                         n_informative=15, n_redundant=2, random_state=42)\n",
        "\n",
        "## 2. Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "## 3. Train Logistic Regression with custom regularization (C=0.5)\n",
        "# Note: In scikit-learn, C is the inverse of regularization strength\n",
        "# Smaller C means stronger regularization\n",
        "model = LogisticRegression(C=0.5, max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "## 4. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "## 5. Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "## 6. Compare with default Logistic Regression (C=1.0)\n",
        "default_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "default_model.fit(X_train, y_train)\n",
        "default_accuracy = accuracy_score(y_test, default_model.predict(X_test))\n",
        "\n",
        "print(f\"\\nAccuracy with default C=1.0: {default_accuracy:.4f}\")\n",
        "print(f\"Difference: {accuracy - default_accuracy:.4f}\")\n",
        "\n",
        "## 7. Visualize the effect of different C values\n",
        "C_values = [0.001, 0.01, 0.1, 0.5, 1.0, 10.0, 100.0]\n",
        "accuracies = []\n",
        "\n",
        "for c in C_values:\n",
        "    lr = LogisticRegression(C=c, max_iter=1000, random_state=42)\n",
        "    lr.fit(X_train, y_train)\n",
        "    accuracies.append(accuracy_score(y_test, lr.predict(X_test)))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(C_values, accuracies, marker='o')\n",
        "plt.axvline(x=0.5, color='r', linestyle='--', label='Our C=0.5')\n",
        "plt.xlabel('Regularization Strength (C)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Logistic Regression Accuracy vs Regularization Strength')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OTmSZJ19takn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients."
      ],
      "metadata": {
        "id": "YlpB9ApqtbO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (important for coefficient interpretation)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "print(f\"Model Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Get feature importance (coefficients)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Coefficient': model.coef_[0],\n",
        "    'Absolute_Coefficient': np.abs(model.coef_[0])\n",
        "})\n",
        "\n",
        "# Sort by absolute coefficient value (most important first)\n",
        "feature_importance = feature_importance.sort_values('Absolute_Coefficient', ascending=False)\n",
        "\n",
        "# Display top 10 most important features\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10)[['Feature', 'Coefficient']])\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(feature_importance['Feature'][:15],\n",
        "         feature_importance['Coefficient'][:15])\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Logistic Regression Feature Importance (Top 15)')\n",
        "plt.gca().invert_yaxis()  # Most important at top\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpretation helper\n",
        "print(\"\\nInterpretation Guide:\")\n",
        "print(\"- Positive coefficients increase the probability of the positive class\")\n",
        "print(\"- Negative coefficients decrease the probability of the positive class\")\n",
        "print(\"- Larger absolute values indicate stronger influence\")"
      ],
      "metadata": {
        "id": "51XsB6pbteZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "HJKGi7RItyw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score, classification_report, confusion_matrix\n",
        "\n",
        "## 1. Generate synthetic classification data (binary)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                         n_informative=15, weights=[0.8, 0.2],  # Imbalanced classes\n",
        "                         random_state=42)\n",
        "\n",
        "## 2. Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                                                   random_state=42, stratify=y)\n",
        "\n",
        "## 3. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, class_weight='balanced',  # Handle imbalance\n",
        "                         random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "## 4. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "## 5. Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.3f}\")\n",
        "\n",
        "## 6. Interpret Kappa Score\n",
        "print(\"\\nKappa Score Interpretation:\")\n",
        "print(\"0.00-0.20 : Slight agreement\")\n",
        "print(\"0.21-0.40 : Fair agreement\")\n",
        "print(\"0.41-0.60 : Moderate agreement\")\n",
        "print(\"0.61-0.80 : Substantial agreement\")\n",
        "print(\"0.81-1.00 : Almost perfect agreement\")\n",
        "\n",
        "## 7. Additional performance metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "## 8. Visualize predictions vs actual\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm', alpha=0.6,\n",
        "           label='Actual', edgecolors='k')\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', marker='x',\n",
        "           s=100, label='Predicted')\n",
        "plt.title(\"Actual vs Predicted Classes\\n(Cohen's κ = {:.2f})\".format(kappa))\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lv9_vcQ0t0wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio:"
      ],
      "metadata": {
        "id": "t-TPsVsRt1qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Step 1: Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
        "                           n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Step 2: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get prediction scores\n",
        "y_scores = model.decision_function(X_test)\n",
        "\n",
        "# Step 5: Compute precision, recall, and average precision score\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Step 6: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'Logistic Regression (AP = {avg_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QyfyKQbyt58N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "Qh43VYVcuD6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=5,\n",
        "                           n_redundant=2, random_state=42)\n",
        "\n",
        "# Step 2: Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracy_results = {}\n",
        "\n",
        "# Step 4: Train and evaluate Logistic Regression with each solver\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[solver] = acc\n",
        "    print(f\"Solver: {solver}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Optional: visualize results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(accuracy_results.keys(), accuracy_results.values(), color=['skyblue', 'salmon', 'lightgreen'])\n",
        "plt.title('Accuracy Comparison of Logistic Regression Solvers')\n",
        "plt.xlabel('Solver')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.8, 1.0)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XiZQvRWFuH8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "czWtLZY0uI7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
        "\n",
        "# Step 1: Generate binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=5,\n",
        "                           n_redundant=2, random_state=42)\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate MCC\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Step 6: Display results\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", round(mcc, 4))\n",
        "\n",
        "# Optional: Print confusion matrix for insight\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "T3HzUiINuOXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Step 1: Create synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=5,\n",
        "                           n_redundant=2, random_state=42)\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "id": "EaA8aJILvmGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "iL8lKjrHult0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=5,\n",
        "                           n_redundant=2, random_state=42)\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(solver='liblinear')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Step 4: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Compare accuracy\n",
        "print(f\"Accuracy without scaling:     {acc_raw:.4f}\")\n",
        "print(f\"Accuracy with scaling:        {acc_scaled:.4f}\")\n",
        "\n",
        "# Optional: Visualize the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = ['Raw Data', 'Standardized Data']\n",
        "accuracies = [acc_raw, acc_scaled]\n",
        "\n",
        "plt.bar(labels, accuracies, color=['orange', 'teal'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Impact of Feature Scaling on Logistic Regression')\n",
        "plt.ylim(0.8, 1.0)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MJcgdswfuoG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation."
      ],
      "metadata": {
        "id": "h2_QWe2WuowJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=5,\n",
        "                           n_redundant=2, random_state=42)\n",
        "\n",
        "# Step 2: Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define parameter grid for C\n",
        "param_grid = {'C': np.logspace(-4, 4, 10)}  # Values from 1e-4 to 1e4\n",
        "\n",
        "# Step 4: Set up Logistic Regression with GridSearchCV\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(\"Best C value found:\", grid_search.best_params_['C'])\n",
        "print(\"Test set accuracy with best C:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "id": "8c5-nXszus9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "s51Vfv0Gutop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Generate synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=5,\n",
        "                           n_redundant=2, random_state=42)\n",
        "\n",
        "# Step 2: Split data into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Save the trained model\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "print(\"Model saved to 'logistic_model.joblib'\")\n",
        "\n",
        "# Step 5: Load the model\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "print(\"Model loaded from 'logistic_model.joblib'\")\n",
        "\n",
        "# Step 6: Make predictions with loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Display prediction results\n",
        "print(\"Accuracy of loaded model:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "id": "_MkXIq8su1MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uK1X9RiJuG9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ddDNyvw5sOsp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}